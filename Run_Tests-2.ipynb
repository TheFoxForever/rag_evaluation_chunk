{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f525da8-27b5-47c8-8eee-52b371b244af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "from transformers import pipeline, GPT2ForQuestionAnswering, T5Tokenizer\n",
    "from torch.cuda.amp import autocast\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af3de6d0-e8be-46a6-bd0d-29f7d0e30e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_compare = {\n",
    "    'openai-community': 'gpt2-medium',\n",
    "    # 'FacebookAI': 'roberta-base',\n",
    "    # 'huggingface-distilbert': 'distilbert-base-uncased-distilled-squad',\n",
    "    # 'Intel' : 'Intel/dynamic_tinybert',\n",
    "    # 'google-t5': 'google-t5/t5-base'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a768c935-4153-4973-8c3e-fd8513aeeac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"<YOUR_TOKEN_HERE>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f60d298-0c2e-4220-b6ce-f3be28f519df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma client\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.get_or_create_collection(name=\"documents\",metadata={'ef':2048, \"hnsw:M\":4096})\n",
    "\n",
    "# Function to add documents to the collection\n",
    "def add_documents_to_chromadb(documents, collection):\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        collection.add(ids=[str(doc_id)], documents=[doc])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd25831b-627d-4fa1-9581-a12524c82321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the chunked documents\n",
    "chunked_documents_dir = '/home/ubuntu/Desktop/capstone/chunked_data'\n",
    "chunk_sizes = [128, 256, 512, 1024, 2048]\n",
    "# chunk_sizes = [512, 1024, 2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d976b10a-4acf-49a9-83c1-0c871aec2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, collection, num_docs):\n",
    "    # changing n_results each iterration\n",
    "    num_docs = 2\n",
    "    results = collection.query(query_texts=[query], n_results=num_docs)\n",
    "    retrieved_docs = [doc for sublist in results['documents'] for doc in sublist]\n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f318c69-6b99-408c-b718-7ad3915f724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_and_add_documents(correct_doc, all_docs, num_random_docs, collection, seed):\n",
    "    clear_collection(collection)\n",
    "    random.seed(seed)\n",
    "    documents_to_add = [correct_doc] + random.sample(all_docs, num_random_docs)\n",
    "    add_documents_to_chromadb(documents_to_add, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd7f93c1-3d31-426d-9ffe-67891675c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_collection(collection):\n",
    "    result = collection.get()\n",
    "    document_ids = result.get('ids', [])\n",
    "    if len(document_ids) > 0:\n",
    "        collection.delete(ids=document_ids) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11386cfe-64ec-4c09-9639-614a54e58b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_answer_with_context(pipeline, query, retrieved_docs):\n",
    "    # QA pipeline\n",
    "    # input_text = f\"question:{query}, context:{context}\"\n",
    "    # response = pipeline([query, context], max_nex_tokens=200)\n",
    "    # answer = response['answer'].strip()\n",
    "    # text2textgeneration\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "    input_text = f\"question: {query} context: {context}\"\n",
    "\n",
    "    # pipeline\n",
    "    response = pipeline(input_text, max_new_tokens=200)\n",
    "    answer = response[0]['generated_text'].strip()\n",
    "\n",
    "    \n",
    "    return answer\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce3d3080-0191-41f5-a086-847ae7e74859",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = []\n",
    "for size in chunk_sizes:\n",
    "    size_dir = os.path.join(chunked_documents_dir, str(size))\n",
    "    for filename in os.listdir(size_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(size_dir, filename), 'r', encoding='utf-8') as file:\n",
    "                all_documents.append(file.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aa97019-c3e9-4b9b-82ce-a09dfcabec98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline for gpt2-medium loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   0%|                                     | 1/3043 [00:00<28:55,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa23-213a_chunk_10.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   0%|                                     | 2/3043 [00:00<20:31,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa23-347a_chunk_17.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   0%|                                     | 3/3043 [00:01<18:07,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa23-158a_chunk_33.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   0%|                                     | 4/3043 [00:01<16:50,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa23-278a_chunk_6.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   0%|                                     | 5/3043 [00:01<16:01,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa23-278a_chunk_66.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   0%|                                     | 6/3043 [00:02<15:27,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa20-301a_chunk_23.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   0%|                                     | 7/3043 [00:02<15:10,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa24-109a_chunk_24.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   0%|                                     | 8/3043 [00:02<15:13,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for ar23-209a_chunk_3.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   0%|                                     | 9/3043 [00:02<15:26,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for icsa-24-074-11_chunk_1.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   0%|                                    | 10/3043 [00:03<15:23,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for ar23-243a_chunk_17.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   0%|▏                                   | 11/3043 [00:03<14:35,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for ar23-243a_chunk_44.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   0%|▏                                   | 13/3043 [00:03<12:34,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa23-263a_chunk_10.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n",
      "Error generating answer for aa23-144a_chunk_36.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   0%|▏                                   | 14/3043 [00:04<12:00,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa23-074a_chunk_35.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   0%|▏                                   | 15/3043 [00:04<12:06,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa22-320a_chunk_9.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   1%|▏                                   | 16/3043 [00:04<12:24,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa20-245a_chunk_22.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   1%|▏                                   | 17/3043 [00:04<12:00,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa21-062a_chunk_6.txt with model gpt2-medium: local variable 'truncated_context' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 128:   1%|▏                                   | 17/3043 [00:05<15:16,  3.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m reset_and_add_documents(chunk, all_documents, num_random_docs, collection, seed)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Retrieve documents and generate the answer\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m retrieved_docs \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m retrieved_docs:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo documents retrieved for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with chunk size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m docs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mretrieve_documents\u001b[0;34m(query, collection, num_docs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_documents\u001b[39m(query, collection, num_docs):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# changing n_results each iterration\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     num_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     retrieved_docs \u001b[38;5;241m=\u001b[39m [doc \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m sublist]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retrieved_docs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/chromadb/api/models/Collection.py:327\u001b[0m, in \u001b[0;36mCollection.query\u001b[0;34m(self, query_embeddings, query_texts, query_images, query_uris, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_query_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         valid_query_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_query_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m         valid_query_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mvalid_query_images)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/chromadb/api/models/Collection.py:633\u001b[0m, in \u001b[0;36mCollection._embed\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must provide an embedding function to compute embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.trychroma.com/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    632\u001b[0m     )\n\u001b[0;32m--> 633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/chromadb/api/types.py:193\u001b[0m, in \u001b[0;36mEmbeddingFunction.__init_subclass__.<locals>.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m: EmbeddingFunction[D], \u001b[38;5;28minput\u001b[39m: D) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[0;32m--> 193\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate_embeddings(maybe_cast_one_to_many_embedding(result))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/chromadb/utils/embedding_functions.py:538\u001b[0m, in \u001b[0;36mONNXMiniLM_L6_V2.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Documents) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;66;03m# Only download the model when it is actually used\u001b[39;00m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_model_if_not_exists()\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Embeddings, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/chromadb/utils/embedding_functions.py:484\u001b[0m, in \u001b[0;36mONNXMiniLM_L6_V2._forward\u001b[0;34m(self, documents, batch_size)\u001b[0m\n\u001b[1;32m    482\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m model_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Perform mean pooling with attention weighting\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m input_mask_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_hidden_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(last_hidden_state \u001b[38;5;241m*\u001b[39m input_mask_expanded, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(\n\u001b[1;32m    488\u001b[0m     input_mask_expanded\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m), a_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-9\u001b[39m, a_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    489\u001b[0m )\n\u001b[1;32m    490\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize(embeddings)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/stride_tricks.py:413\u001b[0m, in \u001b[0;36mbroadcast_to\u001b[0;34m(array, shape, subok)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_broadcast_to_dispatcher, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbroadcast_to\u001b[39m(array, shape, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    369\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Broadcast an array to a new shape.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m           [1, 2, 3]])\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreadonly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/stride_tricks.py:349\u001b[0m, in \u001b[0;36m_broadcast_to\u001b[0;34m(array, shape, subok, readonly)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall elements of broadcast shape must be non-\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    347\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    348\u001b[0m extras \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 349\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnditer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmulti_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrefs_ok\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzerosize_ok\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mextras\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_flags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreadonly\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitershape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m it:\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;66;03m# never really has writebackifcopy semantics\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     broadcast \u001b[38;5;241m=\u001b[39m it\u001b[38;5;241m.\u001b[39mitviews[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 498\n",
    "\n",
    "for model_key, model_path in models_to_compare.items():\n",
    "    model_name = model_path  # Use the value from the dictionary for filenames\n",
    "    save_model = model_name.split(\"/\")\n",
    "    save_model = save_model[-1]\n",
    "    # Initialize pipeline, using model and tokenizer based on its model// device = 0 utilizes the GPU \n",
    "    # nlp_pipeline = pipeline('question-answering', model=model_path, tokenizer=model_path, device=0)\n",
    "    # test t5-flan with text-gerneration\n",
    "    nlp_pipeline = pipeline('text2text-generation', model=model_path, tokenizer=model_path, device=0)\n",
    "\n",
    "    print(f\"Pipeline for {model_name} loaded.\")\n",
    "\n",
    "    for size in chunk_sizes:\n",
    "        size_dir = os.path.join(chunked_documents_dir, str(size))\n",
    "        output_json = f'/home/ubuntu/Desktop/capstone/{save_model}_q_a_{size}.json'\n",
    "        # Load existing JSON if it exists\n",
    "        if os.path.exists(output_json):\n",
    "            with open(output_json, 'r', encoding='utf-8') as file:\n",
    "                output_data = json.load(file)\n",
    "        else:\n",
    "            output_data = []\n",
    "\n",
    "        # Pull questions from CSV so all models receive the same question\n",
    "        input_csv = f'/home/ubuntu/Desktop/capstone/cleaned_questions/generated_questions_{size}.csv'\n",
    "        if os.path.exists(input_csv):\n",
    "            df = pd.read_csv(input_csv)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=['Filename', 'Chunk Size', 'Generated Question', 'Generated Answer'])\n",
    "\n",
    "        # Generate answers for each question\n",
    "        for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=f'Processing {model_name} for chunk size {size}'):\n",
    "            filename = row['Filename']\n",
    "            if isinstance(filename, str) and filename.endswith('.txt'):\n",
    "                with open(os.path.join(size_dir, filename), 'r', encoding='utf-8') as file:\n",
    "                    chunk = file.read()\n",
    "\n",
    "                try:\n",
    "                    question = row['Generated Question']\n",
    "                    entry = {\n",
    "                        'Filename': filename,\n",
    "                        'Chunk Size': size,\n",
    "                        'Generated Question': question,\n",
    "                        'Answers': {}\n",
    "                    }\n",
    "                    # Ask question based on different amounts of docs in vectorstore\n",
    "                    # for num_docs in [1, 3, 5, 10]:\n",
    "                    for num_docs in [3, 5, 10]:\n",
    "                        if num_docs > 1:\n",
    "                            num_random_docs = num_docs - 1\n",
    "                        else:\n",
    "                            num_random_docs = 0\n",
    "\n",
    "                        # Reset and add documents to ChromaDB\n",
    "                        reset_and_add_documents(chunk, all_documents, num_random_docs, collection, seed)\n",
    "\n",
    "                        # Retrieve documents and generate the answer\n",
    "                        retrieved_docs = retrieve_documents(question, collection, num_docs)\n",
    "                        if not retrieved_docs:\n",
    "                            print(f\"No documents retrieved for {filename} with chunk size {size} using model {save_model} and {num_docs} docs\")\n",
    "                            continue\n",
    "\n",
    "                        answer = generate_answer_with_context(nlp_pipeline, question, retrieved_docs)\n",
    "                        entry['Answers'][f'{num_docs} Docs'] = answer\n",
    "                        # print(f\"Generated answer for {filename} with chunk size {size} using model {model_name} and {num_docs} docs\")\n",
    "\n",
    "                    output_data.append(entry)\n",
    "\n",
    "                    # Save the updated JSON after each answer is generated\n",
    "                    with open(output_json, 'w', encoding='utf-8') as file:\n",
    "                        json.dump(output_data, file, indent=4)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating answer for {filename} with model {model_name}: {e}\")\n",
    "\n",
    "        print(f\"Updated answers saved to {output_json} for model {model_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a3739-9a88-46dc-bdab-cc147c65c598",
   "metadata": {},
   "source": [
    "First runs prompt:\n",
    "input_text = f\"Read the following context and answer the question using only information within the context.\\\n",
    "Start your answer with 'Answer':\\n\\n{context}\\n\\nQuestion: {query}\"\n",
    "with top n = num_docs == all document contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e40957-38db-4a0e-b780-59a6e8d9f68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd47a7a-0298-4316-b425-b862f7e51f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70608207-cad3-4ada-b4ec-7119f5a2b774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
