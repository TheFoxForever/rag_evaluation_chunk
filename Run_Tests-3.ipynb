{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f525da8-27b5-47c8-8eee-52b371b244af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.cuda.amp import autocast\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af3de6d0-e8be-46a6-bd0d-29f7d0e30e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_compare = {\n",
    "    # done # 'huggingface-distilbert': {'path': 'distilbert-base-uncased-distilled-squad', 'pipeline': 'question-answering'},\n",
    "    # done # 'Intel' : {'path': 'Intel/dynamic_tinybert', 'pipeline': 'question-answering'},\n",
    "    'Google-t5' : {'path' : 'google-t5/t5-base', 'pipeline': 'text2text-generation'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a768c935-4153-4973-8c3e-fd8513aeeac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"<YOUR_TOKEN_HERE>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f60d298-0c2e-4220-b6ce-f3be28f519df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma client\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.get_or_create_collection(name=\"documents\",metadata={'ef':2048, \"hnsw:M\":4096})\n",
    "\n",
    "# Function to add documents to the collection\n",
    "def add_documents_to_chromadb(documents, collection):\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        collection.add(ids=[str(doc_id)], documents=[doc])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd25831b-627d-4fa1-9581-a12524c82321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the chunked documents\n",
    "chunked_documents_dir = '/home/ubuntu/Desktop/capstone/chunked_data'\n",
    "# chunk_sizes = [128, 256, 512, 1024, 2048]\n",
    "# chunk_sizes = [1024, 2048]\n",
    "chunk_sizes = [256, 512, 1024, 2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d976b10a-4acf-49a9-83c1-0c871aec2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, collection, num_docs):\n",
    "    # changing n_results each iterration\n",
    "    num_docs = 2\n",
    "    results = collection.query(query_texts=[query], n_results=num_docs)\n",
    "    retrieved_docs = [doc for sublist in results['documents'] for doc in sublist]\n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f318c69-6b99-408c-b718-7ad3915f724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_and_add_documents(correct_doc, all_docs, num_random_docs, collection, seed):\n",
    "    clear_collection(collection)\n",
    "    random.seed(seed)\n",
    "    documents_to_add = [correct_doc] + random.sample(all_docs, num_random_docs)\n",
    "    add_documents_to_chromadb(documents_to_add, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd7f93c1-3d31-426d-9ffe-67891675c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_collection(collection):\n",
    "    result = collection.get()\n",
    "    document_ids = result.get('ids', [])\n",
    "    if len(document_ids) > 0:\n",
    "        collection.delete(ids=document_ids) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11386cfe-64ec-4c09-9639-614a54e58b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_answer_with_qa_pipeline(pipeline, query, retrieved_docs, max_length=512):\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "    input_text = f\"question: {query} context: {context}\"\n",
    "\n",
    "    # Tokenize the input and check the length\n",
    "    inputs = pipeline.tokenizer(input_text, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "\n",
    "    # Manually truncate the context if it exceeds the max length\n",
    "    if inputs['input_ids'].shape[1] > max_length:\n",
    "        # Truncate context to fit within max_length considering the query length\n",
    "        query_length = len(pipeline.tokenizer(query, return_tensors='pt')['input_ids'][0])\n",
    "        max_context_length = max_length - query_length\n",
    "        context_tokens = pipeline.tokenizer(context, return_tensors='pt', truncation=True, max_length=max_context_length)\n",
    "        truncated_context = pipeline.tokenizer.decode(context_tokens['input_ids'][0], skip_special_tokens=True)\n",
    "        input_text = f\"question: {query} context: {truncated_context}\"\n",
    "    else:\n",
    "        truncated_context = input_text\n",
    "        \n",
    "    # Generate the answer using the pipeline\n",
    "    response = pipeline(question=query, context=truncated_context)\n",
    "    answer = response['answer'].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# generate answer with context using text-generation pipeline\n",
    "def generate_answer_with_text2text_generation_pipeline(pipeline, query, retrieved_docs, max_length=512, max_new_tokens=100):\n",
    "    # text2textgeneration\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "    input_text = f\"question: {query} context: {context}\"\n",
    "\n",
    "    response = pipeline(input_text, max_new_tokens=max_new_tokens)\n",
    "    answer = response[0]['generated_text'].strip()\n",
    "\n",
    "    \n",
    "    return answer\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce3d3080-0191-41f5-a086-847ae7e74859",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = []\n",
    "for size in chunk_sizes:\n",
    "    size_dir = os.path.join(chunked_documents_dir, str(size))\n",
    "    for filename in os.listdir(size_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(size_dir, filename), 'r', encoding='utf-8') as file:\n",
    "                all_documents.append(file.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aa97019-c3e9-4b9b-82ce-a09dfcabec98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline for gpt2-medium loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt2-medium for chunk size 256:   0%|       | 0/1040 [00:00<?, ?it/s]Keyword arguments {'pad_token_id': 50256} not recognized.\n",
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n",
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n",
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2181 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa23-213a_chunk_10.txt with model gpt2-medium: The size of tensor a (1024) must match the size of tensor b (2181) at non-singleton dimension 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n",
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa23-347a_chunk_17.txt with model gpt2-medium: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n",
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa23-278a_chunk_6.txt with model gpt2-medium: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n",
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n",
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for ar23-209a_chunk_3.txt with model gpt2-medium: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for icsa-24-074-11_chunk_1.txt with model gpt2-medium: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n",
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for ar23-243a_chunk_17.txt with model gpt2-medium: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n",
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa23-263a_chunk_10.txt with model gpt2-medium: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n",
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa22-320a_chunk_9.txt with model gpt2-medium: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n",
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n",
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer for aa21-062a_chunk_6.txt with model gpt2-medium: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'pad_token_id': 50256} not recognized.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing gpt2-medium for chunk size 256:   1%| | 9/1040 [00:06<12:28,  1.38it/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1149\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m-> 1149\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1056\u001b[0m, in \u001b[0;36mPipeline._ensure_tensor_on_device\u001b[0;34m(self, inputs, device)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, UserDict):\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserDict({name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(tensor, device) \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1056\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, UserDict):\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserDict({name: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1062\u001b[0m, in \u001b[0;36mPipeline._ensure_tensor_on_device\u001b[0;34m(self, inputs, device)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m-> 1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo documents retrieved for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with chunk size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m docs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnlp_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieved_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswers\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Docs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m answer\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# print(f\"Generated answer for {filename} with chunk size {size} using model {model_name} and {num_docs} docs\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 47\u001b[0m, in \u001b[0;36mgenerate_text_with_gpt2_pipeline\u001b[0;34m(pipeline, tokenizer, query, retrieved_docs, max_length, max_new_tokens)\u001b[0m\n\u001b[1;32m     44\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39mmax_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Generate the text using the pipeline\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated_text\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:263\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1243\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1236\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1237\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         )\n\u001b[1;32m   1241\u001b[0m     )\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1250\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1249\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1250\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1149\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1147\u001b[0m inference_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_inference_context()\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m-> 1149\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1151\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 498\n",
    "\n",
    "# Process each chunk size separately for each model\n",
    "for model_key, model_info in models_to_compare.items():\n",
    "    model_name = model_info['path']\n",
    "    model_save = model_name.split('/')[-1]\n",
    "    pipeline_type = model_info['pipeline']\n",
    "\n",
    "    # Initialize the correct pipeline based on the model type\n",
    "    if pipeline_type == 'question-answering':\n",
    "        nlp_pipeline = pipeline(pipeline_type, model=model_name, tokenizer=model_name, device=0)\n",
    "        generate_answer = generate_answer_with_qa_pipeline\n",
    "    \n",
    "    else:\n",
    "        nlp_pipeline = pipeline(pipeline_type, model=model_name, tokenizer=model_name, device=0)\n",
    "        generate_answer = generate_answer_with_text2text_generation_pipeline\n",
    "\n",
    "    print(f\"Pipeline for {model_name} loaded.\")\n",
    "\n",
    "    for size in chunk_sizes:\n",
    "        size_dir = os.path.join(chunked_documents_dir, str(size))\n",
    "        output_json = f'/home/ubuntu/Desktop/capstone/2-{model_save}_q_a_{size}.json'\n",
    "        # Load existing JSON if it exists\n",
    "        if os.path.exists(output_json):\n",
    "            with open(output_json, 'r', encoding='utf-8') as file:\n",
    "                output_data = json.load(file)\n",
    "        else:\n",
    "            output_data = []\n",
    "\n",
    "        # Load existing CSV into DataFrame\n",
    "        input_csv = f'/home/ubuntu/Desktop/capstone/cleaned_questions/generated_questions_{size}.csv'\n",
    "        if os.path.exists(input_csv):\n",
    "            df = pd.read_csv(input_csv)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=['Filename', 'Chunk Size', 'Generated Question', 'Generated Answer'])\n",
    "\n",
    "        # Generate answers for all entries\n",
    "        for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=f'Processing {model_name} for chunk size {size}', miniters=10):\n",
    "            filename = row['Filename']\n",
    "            if isinstance(filename, str) and filename.endswith('.txt'):\n",
    "                with open(os.path.join(size_dir, filename), 'r', encoding='utf-8') as file:\n",
    "                    chunk = file.read()\n",
    "\n",
    "                try:\n",
    "                    question = row['Generated Question']\n",
    "                    entry = {\n",
    "                        'Filename': filename,\n",
    "                        'Chunk Size': size,\n",
    "                        'Generated Question': question,\n",
    "                        'Answers': {}\n",
    "                    }\n",
    "\n",
    "                    # for num_docs in [1, 3, 5, 10]:\n",
    "                    for num_docs in [3, 5, 10]:\n",
    "                        if num_docs > 1:\n",
    "                            num_random_docs = num_docs - 1\n",
    "                        else:\n",
    "                            num_random_docs = 0\n",
    "\n",
    "                        # Reset and add documents to ChromaDB\n",
    "                        reset_and_add_documents(chunk, all_documents, num_random_docs, collection, seed)\n",
    "\n",
    "                        # Retrieve documents and generate the answer\n",
    "                        retrieved_docs = retrieve_documents(question, collection, num_docs)\n",
    "                        if not retrieved_docs:\n",
    "                            print(f\"No documents retrieved for {filename} with chunk size {size} using model {model_name} and {num_docs} docs\")\n",
    "                            continue\n",
    "\n",
    "                        answer = generate_answer(nlp_pipeline, tokenizer, question)\n",
    "                        entry['Answers'][f'{num_docs} Docs'] = answer\n",
    "                        # print(f\"Generated answer for {filename} with chunk size {size} using model {model_name} and {num_docs} docs\")\n",
    "\n",
    "                    output_data.append(entry)\n",
    "\n",
    "                    # Save the updated JSON after each answer is generated\n",
    "                    with open(output_json, 'w', encoding='utf-8') as file:\n",
    "                        json.dump(output_data, file, indent=4)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating answer for {filename} with model {model_name}: {e}\")\n",
    "\n",
    "        print(f\"Updated answers saved to {output_json} for model {model_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a3739-9a88-46dc-bdab-cc147c65c598",
   "metadata": {},
   "source": [
    "First runs prompt:\n",
    "input_text = f\"Read the following context and answer the question using only information within the context.\\\n",
    "Start your answer with 'Answer':\\n\\n{context}\\n\\nQuestion: {query}\"\n",
    "with top n = num_docs == all document contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e40957-38db-4a0e-b780-59a6e8d9f68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd47a7a-0298-4316-b425-b862f7e51f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70608207-cad3-4ada-b4ec-7119f5a2b774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
