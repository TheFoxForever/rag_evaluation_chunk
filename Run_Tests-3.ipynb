{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f525da8-27b5-47c8-8eee-52b371b244af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "from transformers import pipeline, GPT2Tokenizer, T5Tokenizer\n",
    "from torch.cuda.amp import autocast\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af3de6d0-e8be-46a6-bd0d-29f7d0e30e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_to_compare = {\n",
    "#     'openai-community': 'gpt2-medium',\n",
    "#     # 'FacebookAI': 'roberta-base',\n",
    "#     # 'huggingface-distilbert': 'distilbert-base-uncased-distilled-squad',\n",
    "#     # 'Intel' : 'Intel/dynamic_tinybert',\n",
    "#     # 'google-t5': 'google-t5/t5-base'\n",
    "# }\n",
    "\n",
    "models_to_compare = {\n",
    "    # 'openai-community': {'path': 'gpt2-medium', 'pipeline': 'text-generation'},\n",
    "    # 'huggingface-distilbert': {'path': 'distilbert-base-uncased-distilled-squad', 'pipeline': 'question-answering'},\n",
    "    'Intel' : {'path': 'Intel/dynamic_tinybert', 'pipeline': 'question-answering'},\n",
    "    # 'Google-t5' : {'path' : 'google-t5/t5-base', 'pipeline': 'text-generation'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a768c935-4153-4973-8c3e-fd8513aeeac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f60d298-0c2e-4220-b6ce-f3be28f519df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma client\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.get_or_create_collection(name=\"documents\",metadata={'ef':2048, \"hnsw:M\":4096})\n",
    "\n",
    "# Function to add documents to the collection\n",
    "def add_documents_to_chromadb(documents, collection):\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        collection.add(ids=[str(doc_id)], documents=[doc])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd25831b-627d-4fa1-9581-a12524c82321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the chunked documents\n",
    "chunked_documents_dir = '/home/ubuntu/Desktop/capstone/chunked_data'\n",
    "chunk_sizes = [128, 256, 512, 1024, 2048]\n",
    "# chunk_sizes = [1024, 2048]\n",
    "# chunk_sizes = [512, 1024, 2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d976b10a-4acf-49a9-83c1-0c871aec2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, collection, num_docs):\n",
    "    # changing n_results each iterration\n",
    "    num_docs = 2\n",
    "    results = collection.query(query_texts=[query], n_results=num_docs)\n",
    "    retrieved_docs = [doc for sublist in results['documents'] for doc in sublist]\n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f318c69-6b99-408c-b718-7ad3915f724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_and_add_documents(correct_doc, all_docs, num_random_docs, collection, seed):\n",
    "    clear_collection(collection)\n",
    "    random.seed(seed)\n",
    "    documents_to_add = [correct_doc] + random.sample(all_docs, num_random_docs)\n",
    "    add_documents_to_chromadb(documents_to_add, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd7f93c1-3d31-426d-9ffe-67891675c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_collection(collection):\n",
    "    result = collection.get()\n",
    "    document_ids = result.get('ids', [])\n",
    "    if len(document_ids) > 0:\n",
    "        collection.delete(ids=document_ids) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11386cfe-64ec-4c09-9639-614a54e58b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_answer_with_qa_pipeline(pipeline, query, retrieved_docs, max_length=512):\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "    input_text = f\"question: {query} context: {context}\"\n",
    "\n",
    "    # Tokenize the input and check the length\n",
    "    inputs = pipeline.tokenizer(input_text, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "\n",
    "    # Manually truncate the context if it exceeds the max length\n",
    "    if inputs['input_ids'].shape[1] > max_length:\n",
    "        # Truncate context to fit within max_length considering the query length\n",
    "        query_length = len(pipeline.tokenizer(query, return_tensors='pt')['input_ids'][0])\n",
    "        max_context_length = max_length - query_length\n",
    "        context_tokens = pipeline.tokenizer(context, return_tensors='pt', truncation=True, max_length=max_context_length)\n",
    "        truncated_context = pipeline.tokenizer.decode(context_tokens['input_ids'][0], skip_special_tokens=True)\n",
    "        input_text = f\"question: {query} context: {truncated_context}\"\n",
    "    else:\n",
    "        truncated_context = input_text\n",
    "        \n",
    "    # Generate the answer using the pipeline\n",
    "    response = pipeline(question=query, context=truncated_context)\n",
    "    answer = response['answer'].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Function to generate answer with context using text-generation pipeline\n",
    "def generate_answer_with_text_generation_pipeline(pipeline, query, retrieved_docs, max_length=512, max_new_tokens=100):\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "    input_text = f\"Question: {query}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    # Tokenize the input and check the length\n",
    "    inputs = pipeline.tokenizer(query,context, return_tensors='pt', truncation=\"only_second\", max_length=max_length)\n",
    "\n",
    "    # Manually truncate the context if it exceeds the max length\n",
    "    # if inputs['input_ids'].shape[1] > max_length:\n",
    "    #     # Truncate context to fit within max_length considering the query length\n",
    "    #     # query_length = len(pipeline.tokenizer(query, return_tensors='pt')['input_ids'][0])\n",
    "    #     # max_context_length = max_length - query_length\n",
    "    #     # context_tokens = pipeline.tokenizer(context, return_tensors='pt', truncation=True, max_length=max_context_length)\n",
    "    #     # truncated_context = pipeline.tokenizer.decode(context_tokens['input_ids'][0], skip_special_tokens=True)\n",
    "    #     # input_text = f\"Question: {query}\\nContext: {truncated_context}\\nAnswer:\"\n",
    "    #     inputs = \n",
    "    # else:\n",
    "    #     truncated_context = inputs\n",
    "\n",
    "    # Generate the answer using the text-generation pipeline\n",
    "    response = pipeline(inputs, max_new_tokens=max_new_tokens)\n",
    "    answer = response[0]['generated_text'].strip()\n",
    "    \n",
    "    # Extract the answer portion only\n",
    "    answer_start = answer.find('Answer:') + len('Answer:')\n",
    "    answer_text = answer[answer_start:].strip()\n",
    "    \n",
    "    return answer_text\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce3d3080-0191-41f5-a086-847ae7e74859",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = []\n",
    "for size in chunk_sizes:\n",
    "    size_dir = os.path.join(chunked_documents_dir, str(size))\n",
    "    for filename in os.listdir(size_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(size_dir, filename), 'r', encoding='utf-8') as file:\n",
    "                all_documents.append(file.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa97019-c3e9-4b9b-82ce-a09dfcabec98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline for Intel/dynamic_tinybert loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Intel/dynamic_tinybert for chunk size 128:   0%| | 0/3043 [00:00<?, ?You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing Intel/dynamic_tinybert for chunk size 128:  52%|â–Œ| 1593/3043 [34:26<4"
     ]
    }
   ],
   "source": [
    "seed = 498\n",
    "\n",
    "# Process each chunk size separately for each model\n",
    "for model_key, model_info in models_to_compare.items():\n",
    "    model_name = model_info['path']\n",
    "    model_save = model_name.split('/')[-1]\n",
    "    pipeline_type = model_info['pipeline']\n",
    "\n",
    "    # Initialize the correct pipeline based on the model type\n",
    "    if pipeline_type == 'question-answering':\n",
    "        nlp_pipeline = pipeline(pipeline_type, model=model_name, tokenizer=model_name, device=0)\n",
    "        generate_answer = generate_answer_with_qa_pipeline\n",
    "    else:\n",
    "        nlp_pipeline = pipeline(pipeline_type, model=model_name, tokenizer=model_name, device=0)\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        generate_answer = generate_answer_with_text_generation_pipeline\n",
    "\n",
    "    print(f\"Pipeline for {model_name} loaded.\")\n",
    "\n",
    "    for size in chunk_sizes:\n",
    "        size_dir = os.path.join(chunked_documents_dir, str(size))\n",
    "        output_json = f'/home/ubuntu/Desktop/capstone/2-{model_save}_q_a_{size}.json'\n",
    "        # Load existing JSON if it exists\n",
    "        if os.path.exists(output_json):\n",
    "            with open(output_json, 'r', encoding='utf-8') as file:\n",
    "                output_data = json.load(file)\n",
    "        else:\n",
    "            output_data = []\n",
    "\n",
    "        # Load existing CSV into DataFrame\n",
    "        input_csv = f'/home/ubuntu/Desktop/capstone/cleaned_questions/generated_questions_{size}.csv'\n",
    "        if os.path.exists(input_csv):\n",
    "            df = pd.read_csv(input_csv)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=['Filename', 'Chunk Size', 'Generated Question', 'Generated Answer'])\n",
    "\n",
    "        # Generate answers for all entries\n",
    "        for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=f'Processing {model_name} for chunk size {size}', miniters=10):\n",
    "            filename = row['Filename']\n",
    "            if isinstance(filename, str) and filename.endswith('.txt'):\n",
    "                with open(os.path.join(size_dir, filename), 'r', encoding='utf-8') as file:\n",
    "                    chunk = file.read()\n",
    "\n",
    "                try:\n",
    "                    question = row['Generated Question']\n",
    "                    entry = {\n",
    "                        'Filename': filename,\n",
    "                        'Chunk Size': size,\n",
    "                        'Generated Question': question,\n",
    "                        'Answers': {}\n",
    "                    }\n",
    "\n",
    "                    # for num_docs in [1, 3, 5, 10]:\n",
    "                    for num_docs in [3, 5, 10]:\n",
    "                        if num_docs > 1:\n",
    "                            num_random_docs = num_docs - 1\n",
    "                        else:\n",
    "                            num_random_docs = 0\n",
    "\n",
    "                        # Reset and add documents to ChromaDB\n",
    "                        reset_and_add_documents(chunk, all_documents, num_random_docs, collection, seed)\n",
    "\n",
    "                        # Retrieve documents and generate the answer\n",
    "                        retrieved_docs = retrieve_documents(question, collection, num_docs)\n",
    "                        if not retrieved_docs:\n",
    "                            print(f\"No documents retrieved for {filename} with chunk size {size} using model {model_name} and {num_docs} docs\")\n",
    "                            continue\n",
    "\n",
    "                        answer = generate_answer(nlp_pipeline, question, retrieved_docs)\n",
    "                        entry['Answers'][f'{num_docs} Docs'] = answer\n",
    "                        # print(f\"Generated answer for {filename} with chunk size {size} using model {model_name} and {num_docs} docs\")\n",
    "\n",
    "                    output_data.append(entry)\n",
    "\n",
    "                    # Save the updated JSON after each answer is generated\n",
    "                    with open(output_json, 'w', encoding='utf-8') as file:\n",
    "                        json.dump(output_data, file, indent=4)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating answer for {filename} with model {model_name}: {e}\")\n",
    "\n",
    "        print(f\"Updated answers saved to {output_json} for model {model_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a3739-9a88-46dc-bdab-cc147c65c598",
   "metadata": {},
   "source": [
    "First runs prompt:\n",
    "input_text = f\"Read the following context and answer the question using only information within the context.\\\n",
    "Start your answer with 'Answer':\\n\\n{context}\\n\\nQuestion: {query}\"\n",
    "with top n = num_docs == all document contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e40957-38db-4a0e-b780-59a6e8d9f68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd47a7a-0298-4316-b425-b862f7e51f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70608207-cad3-4ada-b4ec-7119f5a2b774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
